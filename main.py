import os
import sys
import cv2
import numpy as np
from ultralytics import YOLO
import tkinter as tk
from tkinter import filedialog, messagebox, simpledialog
from PIL import Image, ImageTk
import threading
import time
import shutil

class FaceSwapApp:
    def __init__(self):
        self.model = None
        self.target_faces = []  # å­˜å‚¨ç›®æ ‡äººè„¸å›¾ç‰‡
        self.target_face_paths = ['1v.png', '2v.png', '3v.png']  # ç›®æ ‡äººè„¸è·¯å¾„åˆ—è¡¨
        # ç¡®ä¿ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•çš„ç»å¯¹è·¯å¾„
        self.target_face_paths = [os.path.join(os.getcwd(), path) for path in self.target_face_paths]
        self.yolo_threshold = 0.5  # YOLOv8çš„ç½®ä¿¡åº¦é˜ˆå€¼
        self.use_yolo = True  # å¼ºåˆ¶ä½¿ç”¨YOLOv8
        self.annotations = []  # å­˜å‚¨æ‰‹åŠ¨æ ‡æ³¨çš„æ•°æ®
        self.current_canvas = None  # å­˜å‚¨å½“å‰canvaså¼•ç”¨
        
    def show_menu(self):
        while True:
            print("\n" + "="*50)
            print(" é¢éƒ¨æ›¿æ¢å·¥å…· - ä¸»èœå• (ä»…YOLOv8)")
            print("="*50)
            print("1. ä¸‹è½½å¹¶åˆå§‹åŒ–YOLOv8æ¨¡å‹")
            print("2. åˆ›å»ºæ‰‹åŠ¨æ ‡æ³¨å·¥å…·ï¼ˆBç«™åˆ†è¾¨ç‡æ”¯æŒï¼‰")
            print("3. è®­ç»ƒYOLOv8æ¨¡å‹ï¼ˆä½¿ç”¨txtæ–‡ä»¶å¤¹æ•°æ®é›†ï¼‰")
            print("4. æµ‹è¯•YOLOv8é˜ˆå€¼")
            print("5. å¼€å§‹é¢éƒ¨æ›¿æ¢å·¥ä½œï¼ˆä½¿ç”¨ç›®æ ‡æˆªå›¾ï¼‰")
            print("6. è®¾ç½®YOLOv8é˜ˆå€¼")
            print("7. é€€å‡º")
            print("="*50)
            
            choice = input("è¯·è¾“å…¥é€‰é¡¹ (1-7): ")
            
            if choice == "1":
                self.download_and_initialize_yolo()
            elif choice == "2":
                self.create_manual_annotation_gui()
            elif choice == "3":
                self.train_yolo_with_txt_folder()
            elif choice == "4":
                self.test_yolo_threshold()
            elif choice == "5":
                self.start_face_swap()
            elif choice == "6":
                self.set_yolo_threshold()
            elif choice == "7":
                print("é€€å‡ºç¨‹åº...")
                break
            else:
                print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")

    def download_and_initialize_yolo(self):
        print("\næ­£åœ¨ä¸‹è½½å’Œåˆå§‹åŒ–YOLOv8æ¨¡å‹...")
        
        # ä¸‹è½½YOLOv8sæ¨¡å‹
        print("1. ä¸‹è½½YOLOv8sæ¨¡å‹...")
        try:
            self.model = YOLO('yolov8s.pt')
            print("âœ… YOLOv8sæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ YOLOv8sæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("å°è¯•ä»Hugging Faceä¸‹è½½...")
            try:
                from huggingface_hub import hf_hub_download
                model_path = hf_hub_download(repo_id="ultralytics/yolov8s", filename="yolov8s.pt")
                self.model = YOLO(model_path)
                print("âœ… ä»Hugging FaceæˆåŠŸä¸‹è½½YOLOv8s")
            except Exception as e:
                print(f"âŒ ä»Hugging Faceä¸‹è½½ä¹Ÿå¤±è´¥: {e}")
                print("è¯·æ‰‹åŠ¨ä¸‹è½½YOLOv8sæ¨¡å‹: https://github.com/ultralytics/assets/releases")
                return
        
        # åŠ è½½ç›®æ ‡äººè„¸å›¾ç‰‡
        print("\n2. åŠ è½½ç›®æ ‡äººè„¸å›¾ç‰‡...")
        self.target_faces = []
        for path in self.target_face_paths:
            if os.path.exists(path):
                img = cv2.imread(path)
                if img is not None:
                    self.target_faces.append(img)
                    print(f"  - æˆåŠŸåŠ è½½ç›®æ ‡å›¾ç‰‡: {path}")
                else:
                    print(f"  - è­¦å‘Š: æ— æ³•è¯»å–ç›®æ ‡å›¾ç‰‡ {path}")
            else:
                print(f"  - è­¦å‘Š: ç›®æ ‡å›¾ç‰‡ {path} ä¸å­˜åœ¨")
        
        if len(self.target_faces) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡äººè„¸å›¾ç‰‡")
            return
        
        print("\nâœ… YOLOv8æ¨¡å‹å’Œç›®æ ‡å›¾ç‰‡åˆå§‹åŒ–å®Œæˆï¼")

    def create_manual_annotation_gui(self):
        print("\nåˆ›å»ºæ‰‹åŠ¨æ ‡æ³¨å·¥å…·ï¼ˆBç«™åˆ†è¾¨ç‡æ”¯æŒï¼‰...")
        
        root = tk.Tk()
        root.title("æ‰‹åŠ¨é¢éƒ¨æ ‡æ³¨å·¥å…·")
        root.geometry("1200x800")
        
        canvas = tk.Canvas(root, width=800, height=600, scrollregion=(0, 0, 1920, 1080))
        canvas.pack(pady=10, fill=tk.BOTH, expand=True)
        
        self.current_canvas = canvas
        
        scrollbar = tk.Scrollbar(root, orient=tk.VERTICAL, command=canvas.yview)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.configure(yscrollcommand=scrollbar.set)
        
        def select_file():
            file_path = filedialog.askopenfilename(
                filetypes=[("Image files", "*.png *.jpg *.jpeg *.bmp")]
            )
            if file_path:
                img = cv2.imread(file_path)
                if img is not None:
                    self.display_image_for_annotation(canvas, img, root, file_path)
        
        btn_select = tk.Button(root, text="é€‰æ‹©å›¾ç‰‡è¿›è¡Œæ‰‹åŠ¨æ ‡æ³¨", command=select_file)
        btn_select.pack(pady=5)
        
        def save_annotations():
            file_path = filedialog.asksaveasfilename(
                defaultextension=".txt",
                filetypes=[("Text files", "*.txt"), ("All files", "*.*")]
            )
            if file_path:
                with open(file_path, 'w') as f:
                    for annotation in self.annotations:
                        f.write(f"{annotation['file']},{annotation['x1']},{annotation['y1']},{annotation['x2']},{annotation['y2']},{annotation['label']}\n")
                messagebox.showinfo("ä¿å­˜æˆåŠŸ", f"æ ‡æ³¨æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
        
        btn_save = tk.Button(root, text="ä¿å­˜æ ‡æ³¨æ•°æ®", command=save_annotations)
        btn_save.pack(pady=5)
        
        def clear_annotations():
            self.annotations = []
            messagebox.showinfo("æ¸…é™¤æˆåŠŸ", "æ‰€æœ‰æ ‡æ³¨æ•°æ®å·²æ¸…é™¤")
        
        btn_clear = tk.Button(root, text="æ¸…é™¤æ ‡æ³¨", command=clear_annotations)
        btn_clear.pack(pady=5)
        
        self.annotation_mode = tk.BooleanVar(value=False)
        chk_annotation = tk.Checkbutton(root, text="å¯ç”¨æ‰‹åŠ¨æ ‡æ³¨æ¨¡å¼", variable=self.annotation_mode)
        chk_annotation.pack(pady=5)
        
        tk.Label(root, text="æ ‡æ³¨æ ‡ç­¾ (å¦‚: OTTO):").pack(pady=5)
        self.annotation_label = tk.Entry(root)
        self.annotation_label.pack(pady=5)
        self.annotation_label.insert(0, "OTTO")
        
        root.mainloop()

    def display_image_for_annotation(self, canvas, img, root, file_path):
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img_rgb)
        
        canvas_width = 800
        canvas_height = 600
        
        img_width, img_height = pil_img.size
        scale = min(canvas_width/img_width, canvas_height/img_height)
        
        if scale < 1:
            new_width = int(img_width * scale)
            new_height = int(img_height * scale)
            pil_img = pil_img.resize((new_width, new_height), Image.LANCZOS)
        
        tk_img = ImageTk.PhotoImage(pil_img)
        
        canvas.create_image(0, 0, anchor=tk.NW, image=tk_img)
        canvas.image = tk_img
        
        self.current_image = {
            'path': file_path,
            'pil_img': pil_img,
            'tk_img': tk_img,
            'original_size': (img_width, img_height)
        }
        
        canvas.bind("<Button-1>", lambda e: self.start_annotation(e))
        canvas.bind("<B1-Motion>", lambda e: self.update_annotation(e))
        canvas.bind("<ButtonRelease-1>", lambda e: self.end_annotation(e))
        
        root.update()

    def start_annotation(self, event):
        if not self.annotation_mode.get():
            return
        self.annotation_start = (event.x, event.y)
        self.annotation_rect = self.current_canvas.create_rectangle(
            event.x, event.y, event.x, event.y,
            outline="red", width=2
        )

    def update_annotation(self, event):
        if not self.annotation_mode.get() or not hasattr(self, 'annotation_rect'):
            return
        self.current_canvas.coords(self.annotation_rect, 
                    self.annotation_start[0], self.annotation_start[1],
                    event.x, event.y)

    def end_annotation(self, event):
        if not self.annotation_mode.get() or not hasattr(self, 'annotation_rect'):
            return
        
        coords = self.current_canvas.coords(self.annotation_rect)
        x1, y1, x2, y2 = coords
        
        if x1 > x2:
            x1, x2 = x2, x1
        if y1 > y2:
            y1, y2 = y2, y1
        
        label = self.annotation_label.get() or "unknown"
        
        annotation = {
            'file': self.current_image['path'],
            'x1': x1,
            'y1': y1,
            'x2': x2,
            'y2': y2,
            'label': label
        }
        self.annotations.append(annotation)
        
        print(f"æ·»åŠ æ ‡æ³¨: {annotation}")
        messagebox.showinfo("æ ‡æ³¨æˆåŠŸ", f"å·²æ·»åŠ æ ‡æ³¨: ({x1}, {y1}) - ({x2}, {y2}) æ ‡ç­¾: {label}")

    def train_yolo_with_txt_folder(self):
        print("\nYOLOv8è®­ç»ƒåŠŸèƒ½ï¼ˆä½¿ç”¨txtæ–‡ä»¶å¤¹æ•°æ®é›†ï¼‰...")
        
        if self.model is None:
            print("æ¨¡å‹æœªåŠ è½½ï¼Œæ­£åœ¨è‡ªåŠ¨åŠ è½½YOLOv8sæ¨¡å‹...")
            try:
                self.model = YOLO('yolov8s.pt')
                print("âœ… YOLOv8sæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                print("è¯·å…ˆè¿è¡Œé€‰é¡¹1åˆå§‹åŒ–æ¨¡å‹")
                return
        
        txt_folder = os.path.join(os.getcwd(), "txt")  # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•çš„ç»å¯¹è·¯å¾„
        print(f"æ£€æŸ¥txtæ–‡ä»¶å¤¹: {os.path.abspath(txt_folder)}")
        
        if not os.path.exists(txt_folder):
            print(f"âŒ txtæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {os.path.abspath(txt_folder)}")
            print("è¯·ç¡®ä¿åœ¨æ ¹ç›®å½•ä¸‹æœ‰txtæ–‡ä»¶å¤¹")
            return
        
        try:
            all_files = os.listdir(txt_folder)
            print(f"txtæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶: {all_files}")
            
            png_files = [f for f in all_files if f.endswith('.png')]
            txt_files = [f for f in all_files if f.endswith('.txt')]
            
            print(f"æ‰¾åˆ° {len(png_files)} ä¸ªpngæ–‡ä»¶: {png_files}")
            print(f"æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶: {txt_files}")
            
            if len(png_files) == 0 or len(txt_files) == 0:
                print("âŒ txtæ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°pngæˆ–txtæ–‡ä»¶")
                return
            
        except Exception as e:
            print(f"âŒ è¯»å–txtæ–‡ä»¶å¤¹æ—¶å‡ºé”™: {e}")
            return
        
        print(f"âœ… æ‰¾åˆ° {len(png_files)} ä¸ªpngæ–‡ä»¶å’Œ {len(txt_files)} ä¸ªtxtæ–‡ä»¶")
        
        temp_dataset_path = os.path.join(os.getcwd(), "temp_yolo_dataset")  # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•çš„ç»å¯¹è·¯å¾„
        if os.path.exists(temp_dataset_path):
            shutil.rmtree(temp_dataset_path)
        
        os.makedirs(temp_dataset_path, exist_ok=True)
        os.makedirs(os.path.join(temp_dataset_path, "images"), exist_ok=True)
        os.makedirs(os.path.join(temp_dataset_path, "labels"), exist_ok=True)
        
        for png_file in png_files:
            src_path = os.path.join(txt_folder, png_file)
            dst_path = os.path.join(temp_dataset_path, "images", png_file)
            
            try:
                shutil.copy2(src_path, dst_path)
                print(f"å¤åˆ¶å›¾ç‰‡: {src_path} -> {dst_path}")
            except Exception as e:
                print(f"âŒ å¤åˆ¶å›¾ç‰‡å¤±è´¥: {e}")
                continue
            
            img = cv2.imread(src_path)
            if img is None:
                print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡: {src_path}")
                continue
            img_h, img_w = img.shape[:2]
            
            txt_file = png_file.replace('.png', '.txt')
            if txt_file in txt_files:
                src_txt_path = os.path.join(txt_folder, txt_file)
                dst_txt_path = os.path.join(temp_dataset_path, "labels", txt_file)
                
                try:
                    with open(src_txt_path, 'r') as f:
                        lines = f.readlines()
                    
                    print(f"å¤„ç†æ ‡æ³¨æ–‡ä»¶: {src_txt_path}")
                    
                    yolo_lines = []
                    for i, line in enumerate(lines):
                        parts = line.strip().split(',')
                        
                        coords = []
                        for p in parts:
                            try:
                                coords.append(float(p))
                            except ValueError:
                                continue
                        
                        if len(coords) >= 4:
                            x1, y1, x2, y2 = coords[:4]
                            
                            x_center = (x1 + x2) / 2 / img_w
                            y_center = (y1 + y2) / 2 / img_h
                            width = (x2 - x1) / img_w
                            height = (y2 - y1) / img_h
                            
                            x_center = max(0, min(1, x_center))
                            y_center = max(0, min(1, y_center))
                            width = max(0, min(1, width))
                            height = max(0, min(1, height))
                            
                            yolo_lines.append(f"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")
                    
                    with open(dst_txt_path, 'w') as f:
                        f.writelines(yolo_lines)
                    print(f"âœ… è½¬æ¢æ ‡æ³¨æ–‡ä»¶: {dst_txt_path} (è½¬æ¢äº† {len(yolo_lines)} ä¸ªæ ‡æ³¨)")
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†æ ‡æ³¨æ–‡ä»¶å¤±è´¥: {e}")
        
        print(f"âœ… å·²åˆ›å»ºä¸´æ—¶YOLOæ•°æ®é›†: {temp_dataset_path}")
        
        # ========== å…³é”®ä¿®å¤ï¼šåˆ›å»ºdata.yamlæ–‡ä»¶ ==========
        data_yaml = os.path.join(temp_dataset_path, "data.yaml")
        
        # è·å–ç»å¯¹è·¯å¾„ï¼Œå¹¶å°†åæ–œæ è½¬æ¢ä¸ºæ­£æ–œæ ï¼ˆYOLOv8æ¨èæ ¼å¼ï¼‰
        abs_dataset_path = os.path.abspath(temp_dataset_path).replace('\\', '/')
        
        with open(data_yaml, 'w', encoding='utf-8') as f:
            f.write(f"""path: {abs_dataset_path}
train: images
val: images
test: images

names:
  0: OTTO
""")
        
        print(f"âœ… åˆ›å»ºdata.yamlæ–‡ä»¶: {data_yaml}")
        print(f"   - æ•°æ®é›†ç»å¯¹è·¯å¾„: {abs_dataset_path}")
        
        print("\næ­£åœ¨å¯åŠ¨YOLOv8è®­ç»ƒ...")
        print("è®­ç»ƒå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        try:
            results = self.model.train(
                data=data_yaml,
                epochs=50,
                imgsz=640,
                batch=16,
                name="custom_face_detection"
            )
            
            print("\nâœ… YOLOv8è®­ç»ƒå®Œæˆï¼")
            print(f"è®­ç»ƒç»“æœä¿å­˜åœ¨: runs/detect/custom_face_detection/")
            
            trained_model_path = os.path.join(os.getcwd(), "trained_yolov8s_custom.pt")  # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•çš„ç»å¯¹è·¯å¾„
            self.model.save(trained_model_path)
            print(f"âœ… è®­ç»ƒå¥½çš„æ¨¡å‹å·²ä¿å­˜åˆ°: {trained_model_path}")
            
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            print("è¯·æ£€æŸ¥:")
            print("1. æ˜¯å¦æœ‰GPUæ”¯æŒ")
            print("2. æ•°æ®é›†æ ¼å¼æ˜¯å¦æ­£ç¡®")
            print("3. è·¯å¾„æƒé™æ˜¯å¦æ­£ç¡®")

    def test_yolo_threshold(self):
        print("\næµ‹è¯•YOLOv8é˜ˆå€¼...")
        print("è¯·é€‰æ‹©è¦æµ‹è¯•çš„å›¾ç‰‡:")
        
        test_img_path = filedialog.askopenfilename(
            filetypes=[("Image files", "*.png *.jpg *.jpeg *.bmp")]
        )
        
        if not test_img_path:
            print("âŒ æœªé€‰æ‹©æµ‹è¯•å›¾ç‰‡")
            return
        
        img = cv2.imread(test_img_path)
        if img is None:
            print("âŒ æ— æ³•è¯»å–å›¾ç‰‡")
            return
        
        print(f"\nå½“å‰YOLOv8ç½®ä¿¡åº¦é˜ˆå€¼: {self.yolo_threshold}")
        print(f"æµ‹è¯•å›¾ç‰‡: {test_img_path}")
        print(f"å›¾ç‰‡å°ºå¯¸: {img.shape[1]}x{img.shape[0]}")
        
        if self.model:
            # ä¸ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œè·å–æ‰€æœ‰æ£€æµ‹ç»“æœ
            results = self.model(img, classes=[0], conf=0.0)  # è®¾ç½®conf=0.0è·å–æ‰€æœ‰æ£€æµ‹ç»“æœ
            
            if len(results) > 0 and len(results[0].boxes) > 0:
                print(f"\næ€»å…±æ£€æµ‹åˆ° {len(results[0].boxes)} ä¸ªå¯èƒ½çš„äººè„¸åŒºåŸŸ:")
                print("="*60)
                
                # æŒ‰ç½®ä¿¡åº¦ä»é«˜åˆ°ä½æ’åº
                boxes = sorted(results[0].boxes, key=lambda x: x.conf[0], reverse=True)
                
                for i, box in enumerate(boxes):
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    confidence = box.conf[0]
                    
                    # åˆ¤æ–­æ˜¯å¦è¶…è¿‡å½“å‰é˜ˆå€¼
                    above_threshold = confidence > self.yolo_threshold
                    status = "âœ… é€šè¿‡" if above_threshold else "âŒ ä½äºé˜ˆå€¼"
                    
                    print(f"\näººè„¸åŒºåŸŸ {i+1} {status}:")
                    print(f"  - ç½®ä¿¡åº¦(ç›¸ä¼¼åº¦): {confidence:.4f} ({confidence*100:.2f}%)")
                    print(f"  - è¾¹ç•Œæ¡†: ({x1}, {y1}) åˆ° ({x2}, {y2})")
                    print(f"  - å®½åº¦: {x2-x1}px, é«˜åº¦: {y2-y1}px")
                    print(f"  - å½“å‰é˜ˆå€¼: {self.yolo_threshold}")
                
                # ç»Ÿè®¡è¶…è¿‡é˜ˆå€¼çš„æ•°é‡
                above_threshold_count = sum(1 for box in boxes if box.conf[0] > self.yolo_threshold)
                print(f"\n" + "="*60)
                print(f"æ€»ç»“: {above_threshold_count}/{len(boxes)} ä¸ªäººè„¸åŒºåŸŸè¶…è¿‡å½“å‰é˜ˆå€¼ {self.yolo_threshold}")
                
                # å¦‚æœæœ‰è¶…è¿‡é˜ˆå€¼çš„ï¼Œæ˜¾ç¤ºæœ€é«˜ç½®ä¿¡åº¦
                if above_threshold_count > 0:
                    max_confidence = max(box.conf[0] for box in boxes if box.conf[0] > self.yolo_threshold)
                    print(f"æœ€é«˜ç½®ä¿¡åº¦: {max_confidence:.4f} ({max_confidence*100:.2f}%)")
            else:
                print("\nâŒ æœªæ£€æµ‹åˆ°ä»»ä½•å¯èƒ½çš„äººè„¸åŒºåŸŸ")
                print("å¯èƒ½çš„åŸå› :")
                print("1. å›¾ç‰‡ä¸­ç¡®å®æ²¡æœ‰äººè„¸")
                print("2. äººè„¸å¤ªå°æˆ–å¤ªæ¨¡ç³Š")
                print("3. å…‰çº¿æ¡ä»¶ä¸ä½³")
                print("4. äººè„¸è§’åº¦ä¸å¸¸è§")
                
                # æä¾›è°ƒæ•´å»ºè®®
                if self.yolo_threshold > 0.3:
                    print(f"\nğŸ’¡ å»ºè®®: å½“å‰é˜ˆå€¼è¾ƒé«˜({self.yolo_threshold})ï¼Œå°è¯•é™ä½é˜ˆå€¼å¯èƒ½ä¼šæ£€æµ‹åˆ°æ›´å¤šäººè„¸")
                    print("   å¯ä»¥ä½¿ç”¨é€‰é¡¹6è°ƒæ•´YOLOv8é˜ˆå€¼")
        else:
            print("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè¿è¡Œé€‰é¡¹1åˆå§‹åŒ–æ¨¡å‹")

    def start_face_swap(self):
        print("\nå¼€å§‹é¢éƒ¨æ›¿æ¢å·¥ä½œï¼ˆä½¿ç”¨ç›®æ ‡æˆªå›¾ï¼‰...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç›®æ ‡äººè„¸å›¾ç‰‡
        if len(self.target_faces) == 0:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡äººè„¸å›¾ç‰‡")
            choice = input("è¯·é€‰æ‹©æ“ä½œ:\n1. æ‰‹åŠ¨é€‰æ‹©ç›®æ ‡äººè„¸å›¾ç‰‡\n2. è¿”å›ä¸»èœå•\nè¯·è¾“å…¥é€‰é¡¹ (1-2): ")
            
            if choice == "1":
                self.load_target_faces()
                if len(self.target_faces) == 0:
                    print("âŒ æœªèƒ½åŠ è½½ä»»ä½•ç›®æ ‡äººè„¸å›¾ç‰‡ï¼Œè¿”å›ä¸»èœå•")
                    return
            else:
                print("è¿”å›ä¸»èœå•")
                return
        
        # æ˜¾ç¤ºå½“å‰åŠ è½½çš„ç›®æ ‡äººè„¸å›¾ç‰‡ä¿¡æ¯
        print(f"\nå½“å‰å·²åŠ è½½ {len(self.target_faces)} å¼ ç›®æ ‡äººè„¸å›¾ç‰‡:")
        for i, face_path in enumerate(self.target_face_paths):
            if i < len(self.target_faces):
                print(f"  {i+1}. {os.path.basename(face_path)}")
        
        # è¯¢é—®æ˜¯å¦éœ€è¦æ·»åŠ æ›´å¤šç›®æ ‡äººè„¸å›¾ç‰‡
        add_more = input("\næ˜¯å¦éœ€è¦æ·»åŠ æ›´å¤šç›®æ ‡äººè„¸å›¾ç‰‡? (y/n): ").lower()
        if add_more == 'y':
            self.load_target_faces()
        
        # é€‰æ‹©è§†é¢‘æ–‡ä»¶
        print("\nè¯·é€‰æ‹©è¦å¤„ç†çš„è§†é¢‘æ–‡ä»¶:")
        video_path = filedialog.askopenfilename(
            filetypes=[("Video files", "*.mp4 *.avi *.mov *.mkv *.wmv")]
        )
        
        if not video_path:
            print("âŒ æœªé€‰æ‹©è§†é¢‘æ–‡ä»¶")
            return
        
        # é€‰æ‹©è¾“å‡ºè·¯å¾„
        print("\nè¯·é€‰æ‹©è¾“å‡ºè§†é¢‘çš„ä¿å­˜ä½ç½®:")
        output_path = filedialog.asksaveasfilename(
            defaultextension=".mp4",
            filetypes=[("MP4 files", "*.mp4"), ("AVI files", "*.avi"), ("All files", "*.*")]
        )
        
        if not output_path:
            print("âŒ æœªé€‰æ‹©è¾“å‡ºè·¯å¾„")
            return
        
        # æ˜¾ç¤ºå¤„ç†ä¿¡æ¯å¹¶å¼€å§‹å¤„ç†
        print(f"\nå‡†å¤‡å¼€å§‹å¤„ç†:")
        print(f"  - è¾“å…¥è§†é¢‘: {video_path}")
        print(f"  - è¾“å‡ºè§†é¢‘: {output_path}")
        print(f"  - å½“å‰YOLOv8é˜ˆå€¼: {self.yolo_threshold}")
        print(f"  - ç›®æ ‡äººè„¸å›¾ç‰‡æ•°é‡: {len(self.target_faces)}")
        
        # è¯¢é—®æ˜¯å¦éœ€è¦è°ƒæ•´é˜ˆå€¼
        print(f"\nå½“å‰YOLOv8ç½®ä¿¡åº¦é˜ˆå€¼ä¸º: {self.yolo_threshold}")
        print("é˜ˆå€¼è¶Šé«˜ï¼Œæ£€æµ‹è¶Šä¸¥æ ¼ä½†å¯èƒ½æ¼æ‰ä¸€äº›äººè„¸")
        print("é˜ˆå€¼è¶Šä½ï¼Œæ£€æµ‹è¶Šå®½æ¾ä½†å¯èƒ½è¯¯æ£€")
        adjust = input("æ˜¯å¦éœ€è¦è°ƒæ•´YOLOv8é˜ˆå€¼? (y/n): ").lower()
        if adjust == 'y':
            try:
                new_threshold = float(input(f"è¯·è¾“å…¥æ–°çš„é˜ˆå€¼ (0.0-1.0, å½“å‰: {self.yolo_threshold}): "))
                if 0.0 <= new_threshold <= 1.0:
                    self.yolo_threshold = new_threshold
                    print(f"âœ… é˜ˆå€¼å·²è°ƒæ•´ä¸º: {self.yolo_threshold}")
                else:
                    print(f"âŒ è¾“å…¥æ— æ•ˆï¼Œä¿æŒåŸé˜ˆå€¼: {self.yolo_threshold}")
            except ValueError:
                print(f"âŒ è¾“å…¥æ— æ•ˆï¼Œä¿æŒåŸé˜ˆå€¼: {self.yolo_threshold}")
        
        # è¯¢é—®æ˜¯å¦éœ€è¦å…ˆæµ‹è¯•é˜ˆå€¼
        test_threshold = input("\næ˜¯å¦éœ€è¦å…ˆæµ‹è¯•å½“å‰é˜ˆå€¼æ•ˆæœ? (y/n): ").lower()
        if test_threshold == 'y':
            self.test_yolo_threshold_on_video(video_path)
        
        confirm = input("\nç¡®è®¤å¼€å§‹å¤„ç†? (y/n): ").lower()
        if confirm != 'y':
            print("å·²å–æ¶ˆå¤„ç†")
            return
        
        print("\nå¼€å§‹å¤„ç†è§†é¢‘...")
        self.process_video(video_path, output_path)
    
    def test_yolo_threshold_on_video(self, video_path):
        """åœ¨è§†é¢‘ä¸Šæµ‹è¯•å½“å‰é˜ˆå€¼æ•ˆæœ"""
        print(f"\næ­£åœ¨è§†é¢‘ {video_path} ä¸Šæµ‹è¯•é˜ˆå€¼æ•ˆæœ...")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        test_frames = min(10, total_frames)  # å…ˆæµ‹è¯•10å¸§è¿›è¡Œè¯¦ç»†åˆ†æ
        frame_step = max(1, total_frames // test_frames)
        
        detection_count = 0
        total_detections = 0
        all_detections = []  # å­˜å‚¨æ‰€æœ‰æ£€æµ‹ç»“æœï¼ˆåŒ…æ‹¬ä½äºé˜ˆå€¼çš„ï¼‰
        
        print(f"å°†è¯¦ç»†åˆ†æ {test_frames} å¸§æ¥è¯„ä¼°æ£€æµ‹æ•ˆæœ...")
        
        for i in range(test_frames):
            frame_idx = i * frame_step
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if not ret:
                continue
            
            print(f"\nåˆ†æå¸§ {frame_idx}:")
            
            if self.model:
                # è·å–æ‰€æœ‰æ£€æµ‹ç»“æœï¼Œä¸ä½¿ç”¨é˜ˆå€¼è¿‡æ»¤
                results = self.model(frame, classes=[0], conf=0.0)
                
                frame_detections = 0
                frame_all_detections = 0
                
                for result in results:
                    boxes = result.boxes
                    for box in boxes:
                        confidence = box.conf[0]
                        frame_all_detections += 1
                        all_detections.append(confidence)
                        
                        print(f"  - æ£€æµ‹åˆ°äººè„¸ï¼Œç½®ä¿¡åº¦: {confidence:.4f}")
                        
                        if confidence > self.yolo_threshold:
                            frame_detections += 1
                            total_detections += 1
                
                if frame_all_detections == 0:
                    print("  - æœªæ£€æµ‹åˆ°ä»»ä½•äººè„¸åŒºåŸŸ")
                    print("  å¯èƒ½çš„åŸå› :")
                    print("    1. è§†é¢‘ä¸­ç¡®å®æ²¡æœ‰äººè„¸")
                    print("    2. äººè„¸å¤ªå°æˆ–å¤ªæ¨¡ç³Š")
                    print("    3. å…‰çº¿æ¡ä»¶ä¸ä½³")
                    print("    4. äººè„¸è§’åº¦ä¸å¸¸è§")
                    print("    5. æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
                else:
                    detection_count += 1
                    print(f"  - è¶…è¿‡å½“å‰é˜ˆå€¼ {self.yolo_threshold} çš„æ£€æµ‹æ•°: {frame_detections}")
        
        cap.release()
        
        print(f"\nè¯¦ç»†æ£€æµ‹ç»“æœ:")
        print(f"  - æµ‹è¯•å¸§æ•°: {test_frames}")
        print(f"  - æœ‰äººè„¸çš„å¸§æ•°: {detection_count}")
        print(f"  - æ€»æ£€æµ‹æ¬¡æ•°: {total_detections}")
        print(f"  - æ€»æ£€æµ‹åŒºåŸŸæ•°ï¼ˆåŒ…æ‹¬ä½ç½®ä¿¡åº¦ï¼‰: {len(all_detections)}")
        
        if len(all_detections) > 0:
            print(f"  - æœ€é«˜ç½®ä¿¡åº¦: {max(all_detections):.4f}")
            print(f"  - æœ€ä½ç½®ä¿¡åº¦: {min(all_detections):.4f}")
            print(f"  - å¹³å‡ç½®ä¿¡åº¦: {sum(all_detections)/len(all_detections):.4f}")
            
            # æä¾›é˜ˆå€¼å»ºè®®
            print(f"\nğŸ’¡ é˜ˆå€¼è°ƒæ•´å»ºè®®:")
            if max(all_detections) < self.yolo_threshold:
                print(f"  - å½“å‰é˜ˆå€¼ {self.yolo_threshold} é«˜äºæœ€é«˜æ£€æµ‹ç½®ä¿¡åº¦ {max(all_detections):.4f}")
                print(f"  - å»ºè®®å°†é˜ˆå€¼è®¾ç½®ä¸º: {max(all_detections) * 0.8:.4f}")
            else:
                high_conf_count = sum(1 for c in all_detections if c > 0.5)
                if high_conf_count > 0:
                    print(f"  - æœ‰ {high_conf_count} ä¸ªé«˜ç½®ä¿¡åº¦æ£€æµ‹ (>0.5)ï¼Œå½“å‰é˜ˆå€¼å¯èƒ½åˆé€‚")
                else:
                    print(f"  - æ‰€æœ‰æ£€æµ‹ç½®ä¿¡åº¦éƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥è§†é¢‘è´¨é‡æˆ–ä½¿ç”¨æ›´ä½é˜ˆå€¼")
                    print(f"  - å»ºè®®å°†é˜ˆå€¼è®¾ç½®ä¸º: {max(all_detections) * 0.7:.4f}")
        else:
            print(f"\nâš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•å¯èƒ½çš„äººè„¸åŒºåŸŸ")
            print("å¯èƒ½çš„åŸå› :")
            print("1. è§†é¢‘ä¸­ç¡®å®æ²¡æœ‰äººè„¸")
            print("2. æ¨¡å‹æœªæ­£ç¡®åŠ è½½æˆ–æŸå")
            print("3. è§†é¢‘æ ¼å¼ä¸æ”¯æŒæˆ–æŸå")
            print("4. äººè„¸å¤ªå°ï¼ˆå°äºæ¨¡å‹æœ€å°æ£€æµ‹å°ºå¯¸ï¼‰")
            
            # æä¾›è¿›ä¸€æ­¥è¯Šæ–­é€‰é¡¹
            diagnose = input("\næ˜¯å¦éœ€è¦è¿›è¡Œè¿›ä¸€æ­¥è¯Šæ–­? (y/n): ").lower()
            if diagnose == 'y':
                self.diagnose_model_and_video(video_path)
        
        input("\næŒ‰å›è½¦é”®ç»§ç»­...")
    
    def diagnose_model_and_video(self, video_path):
        """è¿›ä¸€æ­¥è¯Šæ–­æ¨¡å‹å’Œè§†é¢‘é—®é¢˜"""
        print("\n=== è¯¦ç»†è¯Šæ–­ ===")
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        print(f"1. æ£€æŸ¥æ¨¡å‹çŠ¶æ€:")
        if self.model is None:
            print("   âŒ æ¨¡å‹æœªåŠ è½½")
            return
        else:
            print("   âœ… æ¨¡å‹å·²åŠ è½½")
        
        # æ£€æŸ¥è§†é¢‘åŸºæœ¬ä¿¡æ¯
        print(f"\n2. æ£€æŸ¥è§†é¢‘ä¿¡æ¯:")
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("   âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return
        
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"   - åˆ†è¾¨ç‡: {width}x{height}")
        print(f"   - å¸§ç‡: {fps}")
        print(f"   - æ€»å¸§æ•°: {total_frames}")
        
        # å°è¯•è¯»å–ç¬¬ä¸€å¸§
        ret, frame = cap.read()
        if not ret:
            print("   âŒ æ— æ³•è¯»å–è§†é¢‘å¸§")
            cap.release()
            return
        
        print("   âœ… å¯ä»¥æ­£å¸¸è¯»å–è§†é¢‘å¸§")
        
        # ä¿å­˜ç¬¬ä¸€å¸§ä½œä¸ºæµ‹è¯•å›¾ç‰‡
        test_frame_path = os.path.join(os.getcwd(), "test_frame.jpg")
        cv2.imwrite(test_frame_path, frame)
        print(f"   âœ… å·²ä¿å­˜æµ‹è¯•å¸§åˆ°: {test_frame_path}")
        
        cap.release()
        
        # ä½¿ç”¨æ¨¡å‹æµ‹è¯•ç¬¬ä¸€å¸§
        print(f"\n3. ä½¿ç”¨æ¨¡å‹æµ‹è¯•ç¬¬ä¸€å¸§:")
        try:
            results = self.model(frame, conf=0.0)  # ä¸ä½¿ç”¨ç±»åˆ«è¿‡æ»¤ï¼Œæ£€æµ‹æ‰€æœ‰å¯¹è±¡
            print(f"   - æ¨¡å‹æ¨ç†æˆåŠŸ")
            
            total_objects = 0
            for result in results:
                boxes = result.boxes
                total_objects += len(boxes)
                
                for box in boxes:
                    cls = int(box.cls[0])
                    confidence = box.conf[0]
                    class_name = self.model.names[cls] if hasattr(self.model, 'names') else f"Class {cls}"
                    print(f"   - æ£€æµ‹åˆ°: {class_name}, ç½®ä¿¡åº¦: {confidence:.4f}")
            
            if total_objects == 0:
                print("   âš ï¸ æ¨¡å‹æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡")
                print("   å¯èƒ½æ˜¯æ¨¡å‹é—®é¢˜æˆ–è§†é¢‘ä¸­ç¡®å®æ²¡æœ‰æ˜æ˜¾å¯¹è±¡")
            else:
                print(f"   âœ… æ¨¡å‹æ­£å¸¸å·¥ä½œï¼Œå…±æ£€æµ‹åˆ° {total_objects} ä¸ªå¯¹è±¡")
                
                # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°äººè„¸ï¼ˆç±»åˆ«0ï¼‰
                person_detections = sum(1 for result in results for box in result.boxes if int(box.cls[0]) == 0)
                if person_detections == 0:
                    print("   âš ï¸ æœªæ£€æµ‹åˆ°äººè„¸ç±»åˆ«ï¼Œä½†æ£€æµ‹åˆ°å…¶ä»–å¯¹è±¡")
                    print("   å¯èƒ½æ˜¯è§†é¢‘ä¸­ç¡®å®æ²¡æœ‰äººè„¸ï¼Œæˆ–è€…äººè„¸å¤ªå°/ä¸æ¸…æ™°")
        
        except Exception as e:
            print(f"   âŒ æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        print(f"\n4. å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥æµ‹è¯•å¸§å›¾ç‰‡ï¼Œç¡®è®¤è§†é¢‘ä¸­æ˜¯å¦æœ‰äººè„¸")
        print("   2. å¦‚æœæœ‰äººè„¸ä½†æœªæ£€æµ‹åˆ°ï¼Œå°è¯•é™ä½é˜ˆå€¼åˆ°0.1æˆ–æ›´ä½")
        print("   3. ç¡®ä¿äººè„¸åœ¨ç”»é¢ä¸­è¶³å¤Ÿå¤§ï¼ˆå»ºè®®æœ€å°32x32åƒç´ ï¼‰")
        print("   4. å°è¯•ä½¿ç”¨å…¶ä»–è§†é¢‘æµ‹è¯•")
        print("   5. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæˆ–ä¸‹è½½æ¨¡å‹")
    
    def load_target_faces(self):
        """åŠ è½½ç›®æ ‡äººè„¸å›¾ç‰‡"""
        print("\nè¯·é€‰æ‹©ç›®æ ‡äººè„¸å›¾ç‰‡ (å¯å¤šé€‰):")
        
        file_paths = filedialog.askopenfilenames(
            title="é€‰æ‹©ç›®æ ‡äººè„¸å›¾ç‰‡",
            filetypes=[("Image files", "*.png *.jpg *.jpeg *.bmp")]
        )
        
        if not file_paths:
            print("æœªé€‰æ‹©ä»»ä½•å›¾ç‰‡")
            return
        
        loaded_count = 0
        for file_path in file_paths:
            try:
                img = cv2.imread(file_path)
                if img is not None:
                    self.target_faces.append(img)
                    self.target_face_paths.append(file_path)
                    loaded_count += 1
                    print(f"âœ… æˆåŠŸåŠ è½½: {os.path.basename(file_path)}")
                else:
                    print(f"âŒ æ— æ³•è¯»å–: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {os.path.basename(file_path)}: {e}")
        
        print(f"\næ€»å…±åŠ è½½äº† {loaded_count} å¼ ç›®æ ‡äººè„¸å›¾ç‰‡")
        print(f"å½“å‰å…±æœ‰ {len(self.target_faces)} å¼ ç›®æ ‡äººè„¸å›¾ç‰‡å¯ä¾›æ›¿æ¢")

    def process_video(self, video_path, output_path):
        print(f"\næ­£åœ¨æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸåæˆ–æ ¼å¼æ˜¯å¦æ”¯æŒ")
            return
        
        # è·å–è§†é¢‘ä¿¡æ¯
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        print(f"è§†é¢‘ä¿¡æ¯:")
        print(f"  - åˆ†è¾¨ç‡: {width}x{height}")
        print(f"  - å¸§ç‡: {fps:.2f} FPS")
        print(f"  - æ€»å¸§æ•°: {total_frames}")
        print(f"  - æ—¶é•¿: {duration:.2f} ç§’ ({duration/60:.2f} åˆ†é’Ÿ)")
        
        # è®¾ç½®è§†é¢‘ç¼–ç å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if not out.isOpened():
            print("âŒ æ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è¾“å‡ºè·¯å¾„æƒé™")
            cap.release()
            return
        
        frame_idx = 0
        swap_count = 0
        no_detection_count = 0
        max_no_detection = 100  # è¿ç»­100å¸§æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸æ—¶æç¤º
        
        print("\nå¼€å§‹å¤„ç†è§†é¢‘å¸§...")
        print("æç¤º: æŒ‰ Ctrl+C å¯ä»¥ä¸­æ–­å¤„ç†")
        start_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_idx += 1
                
                # æ¯å¤„ç†ä¸€å®šå¸§æ•°æ˜¾ç¤ºè¿›åº¦
                if frame_idx % 30 == 0:
                    elapsed = time.time() - start_time
                    progress = (frame_idx / total_frames) * 100
                    eta = (elapsed / frame_idx) * (total_frames - frame_idx) if frame_idx > 0 else 0
                    print(f"å¤„ç†è¿›åº¦: {frame_idx}/{total_frames} å¸§ ({progress:.1f}%) | "
                          f"å·²æ¢è„¸: {swap_count} æ¬¡ | è€—æ—¶: {elapsed:.1f}s | é¢„è®¡å‰©ä½™: {eta:.1f}s")
                
                if self.model:
                    # ä½¿ç”¨YOLOv8æ£€æµ‹äººè„¸
                    # æ³¨æ„ï¼šYOLOv8é»˜è®¤ä¸ç›´æ¥æ”¯æŒäººè„¸æ£€æµ‹ï¼Œæˆ‘ä»¬ä½¿ç”¨personæ£€æµ‹ç„¶åå®šä½äººè„¸åŒºåŸŸ
                    results = self.model(frame, classes=[0], conf=0.0)  # æ£€æµ‹æ‰€æœ‰personç±»åˆ«
                    
                    detected_faces = 0
                    for result in results:
                        boxes = result.boxes
                        for box in boxes:
                            x1, y1, x2, y2 = map(int, box.xyxy[0])
                            confidence = box.conf[0]
                            
                            # åªå¤„ç†è¶…è¿‡é˜ˆå€¼çš„äººä½“æ£€æµ‹
                            if confidence > self.yolo_threshold:
                                # ä»äººä½“åŒºåŸŸä¼°ç®—äººè„¸ä½ç½®
                                # äººè„¸é€šå¸¸åœ¨äººä½“ä¸Šéƒ¨1/4åŒºåŸŸï¼Œä¸”å±…ä¸­
                                body_height = y2 - y1
                                body_width = x2 - x1
                                
                                # ä¼°ç®—äººè„¸åŒºåŸŸï¼ˆäººä½“ä¸Šéƒ¨1/4ï¼Œå®½åº¦å±…ä¸­1/2ï¼‰
                                face_y1 = y1
                                face_y2 = y1 + int(body_height * 0.35)  # äººè„¸çº¦å èº«ä½“é«˜åº¦çš„35%
                                face_height = face_y2 - face_y1
                                
                                # äººè„¸å®½åº¦çº¦ä¸ºé«˜åº¦çš„80%ï¼Œå±…ä¸­æ”¾ç½®
                                face_width = int(face_height * 0.8)
                                face_x1 = x1 + (body_width - face_width) // 2
                                face_x2 = face_x1 + face_width
                                
                                # ç¡®ä¿äººè„¸åŒºåŸŸåœ¨äººä½“è¾¹ç•Œå†…
                                face_x1 = max(x1, face_x1)
                                face_x2 = min(x2, face_x2)
                                face_y2 = min(y1 + int(body_height * 0.5), face_y2)  # ä¸è¶…è¿‡èº«ä½“ä¸Šéƒ¨50%
                                
                                # è·³è¿‡å¤ªå°çš„äººè„¸åŒºåŸŸ
                                if (face_x2 - face_x1) < 30 or (face_y2 - face_y1) < 30:
                                    print(f"è·³è¿‡å¤ªå°çš„äººè„¸åŒºåŸŸ: {(face_x2-face_x1)}x{(face_y2-face_y1)}")
                                    continue
                                
                                detected_faces += 1
                                
                                # é€‰æ‹©ç›®æ ‡äººè„¸å›¾ç‰‡ï¼ˆå¾ªç¯ä½¿ç”¨ï¼‰
                                target_idx = swap_count % len(self.target_faces)
                                target_img = self.target_faces[target_idx]
                                
                                # è°ƒæ•´ç›®æ ‡å›¾ç‰‡å¤§å°ä»¥åŒ¹é…ä¼°ç®—çš„äººè„¸åŒºåŸŸ
                                try:
                                    resized_target = cv2.resize(target_img, (face_x2 - face_x1, face_y2 - face_y1))
                                    
                                    # åº”ç”¨è¾¹ç¼˜èåˆä½¿æ›¿æ¢æ›´è‡ªç„¶
                                    if (face_x2 - face_x1) > 60 and (face_y2 - face_y1) > 60:  # åªå¯¹è¾ƒå¤§çš„äººè„¸åº”ç”¨èåˆ
                                        # åˆ›å»ºçŸ©å½¢æ©ç ç”¨äºè¾¹ç¼˜èåˆï¼Œä¿æŒåŸå§‹å›¾ç‰‡å½¢çŠ¶
                                        mask = np.ones((face_y2 - face_y1, face_x2 - face_x1), dtype=np.float32)
                                        
                                        # åˆ›å»ºè¾¹ç¼˜æ¸å˜æ•ˆæœï¼Œä½¿æ›¿æ¢æ›´è‡ªç„¶
                                        edge_width = min(15, min(face_x2 - face_x1, face_y2 - face_y1) // 4)  # è¾¹ç¼˜å®½åº¦ä¸ºè¾ƒå°è¾¹é•¿çš„1/4ï¼Œæœ€å¤§15åƒç´ 
                                        
                                        # ä¸Šè¾¹ç¼˜
                                        for i in range(edge_width):
                                            mask[i, :] = i / edge_width
                                        # ä¸‹è¾¹ç¼˜
                                        for i in range(edge_width):
                                            mask[(face_y2 - face_y1) - 1 - i, :] = i / edge_width
                                        # å·¦è¾¹ç¼˜
                                        for i in range(edge_width):
                                            mask[:, i] = np.maximum(mask[:, i], i / edge_width)
                                        # å³è¾¹ç¼˜
                                        for i in range(edge_width):
                                            mask[:, (face_x2 - face_x1) - 1 - i] = np.maximum(mask[:, (face_x2 - face_x1) - 1 - i], i / edge_width)
                                        
                                        # åº”ç”¨é«˜æ–¯æ¨¡ç³Šä½¿è¾¹ç¼˜æ›´å¹³æ»‘
                                        cv2.GaussianBlur(mask, (15, 15), 0)
                                        
                                        # åº”ç”¨æ©ç ï¼Œåªæ›¿æ¢äººè„¸åŒºåŸŸ
                                        for c in range(3):
                                            frame[face_y1:face_y2, face_x1:face_x2, c] = (
                                                frame[face_y1:face_y2, face_x1:face_x2, c] * (1 - mask) + 
                                                resized_target[:, :, c] * mask
                                            )
                                    else:
                                        # å°äººè„¸ç›´æ¥æ›¿æ¢
                                        frame[face_y1:face_y2, face_x1:face_x2] = resized_target
                                    
                                    swap_count += 1
                                    
                                    # åœ¨å¸§ä¸Šç»˜åˆ¶æ›¿æ¢ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
                                    cv2.putText(frame, f"Face {swap_count} (Conf: {confidence:.2f})", 
                                               (face_x1, face_y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                                    
                                    # ç»˜åˆ¶äººè„¸åŒºåŸŸè¾¹ç•Œæ¡†ï¼ˆè°ƒè¯•ç”¨ï¼Œå¯é€‰ï¼‰
                                    cv2.rectangle(frame, (face_x1, face_y1), (face_x2, face_y2), (0, 255, 255), 1)
                                    
                                except Exception as e:
                                    print(f"å¤„ç†äººè„¸æ—¶å‡ºé”™: {e}")
                                    continue
                    
                    # ç»Ÿè®¡æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸çš„å¸§æ•°
                    if detected_faces == 0:
                        no_detection_count += 1
                        if no_detection_count >= max_no_detection:
                            print(f"âš ï¸ å·²è¿ç»­ {no_detection_count} å¸§æœªæ£€æµ‹åˆ°äººè„¸ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é˜ˆå€¼")
                            no_detection_count = 0  # é‡ç½®è®¡æ•°å™¨ï¼Œé¿å…é‡å¤æç¤º
                    else:
                        no_detection_count = 0
                
                # å†™å…¥å¤„ç†åçš„å¸§
                out.write(frame)
        
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­å¤„ç†")
            print(f"å·²å¤„ç† {frame_idx}/{total_frames} å¸§")
        
        # é‡Šæ”¾èµ„æº
        cap.release()
        out.release()
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœ
        elapsed = time.time() - start_time
        avg_fps = frame_idx / elapsed if elapsed > 0 else 0
        
        print(f"\nâœ… è§†é¢‘å¤„ç†å®Œæˆï¼")
        print(f"   - æ€»å¸§æ•°: {frame_idx}/{total_frames}")
        print(f"   - æ¢è„¸æ¬¡æ•°: {swap_count}")
        print(f"   - æ€»è€—æ—¶: {elapsed:.1f}ç§’ ({elapsed/60:.2f}åˆ†é’Ÿ)")
        print(f"   - å¹³å‡å¤„ç†é€Ÿåº¦: {avg_fps:.2f} FPS")
        print(f"   - è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            print(f"âœ… è¾“å‡ºè§†é¢‘æ–‡ä»¶å·²æˆåŠŸåˆ›å»º")
            
            # è¯¢é—®æ˜¯å¦æ’­æ”¾è§†é¢‘
            play = input("\næ˜¯å¦æ’­æ”¾å¤„ç†åçš„è§†é¢‘? (y/n): ").lower()
            if play == 'y':
                self.play_video(output_path)
        else:
            print(f"âŒ è¾“å‡ºè§†é¢‘æ–‡ä»¶åˆ›å»ºå¤±è´¥")
    
    def play_video(self, video_path):
        """ä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ’­æ”¾å™¨æ’­æ”¾è§†é¢‘"""
        try:
            if os.name == 'nt':  # Windows
                os.startfile(video_path)
            elif os.name == 'posix':  # macOS and Linux
                os.system(f'open "{video_path}"' if sys.platform == 'darwin' else f'xdg-open "{video_path}"')
            print(f"æ­£åœ¨æ’­æ”¾è§†é¢‘: {video_path}")
        except Exception as e:
            print(f"æ— æ³•æ’­æ”¾è§†é¢‘: {e}")

    def set_yolo_threshold(self):
        print("\nè®¾ç½®YOLOv8é˜ˆå€¼...")
        print(f"å½“å‰YOLOv8ç½®ä¿¡åº¦é˜ˆå€¼: {self.yolo_threshold}")
        
        try:
            new_threshold = float(input("è¯·è¾“å…¥æ–°çš„YOLOv8ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0): "))
            if 0.0 <= new_threshold <= 1.0:
                self.yolo_threshold = new_threshold
                print(f"âœ… YOLOv8é˜ˆå€¼å·²è®¾ç½®ä¸º: {self.yolo_threshold}")
            else:
                print("âŒ é˜ˆå€¼å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´")
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

if __name__ == "__main__":
    app = FaceSwapApp()
    app.show_menu()